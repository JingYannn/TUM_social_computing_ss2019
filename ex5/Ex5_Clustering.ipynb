{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Social Computing - Summer 2019\n",
    "# Exercise 5 - Clustering\n",
    "Clustering is analyzing data structures (regarding this exercise, especially datasets and graphs) according to certain characteristics of interest and grouping the points whose attributes overlap. This procedure yields new disjoint groupings, the clusters the points belong to, which may reveal insightful information. In the following, we will take a look at two different clustering algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 5.1: k-Means Clustering\n",
    "\n",
    "In the first section of the exercise, you will work with the dataset *NetworkActivityData.csv* which documents the user's activity of an online social network. Each line in the file represents a feature vector and is associated with one of the users. The vectors contain an ID ('userXYZ') and the following four different activity types (features) of that user:\n",
    "* Number of posts\n",
    "* Number of comments\n",
    "* Number of likes (on posts and comments)\n",
    "* Number of friends\n",
    "\n",
    "By clustering the users according to all of the features, you can identify similar activity patterns among them. In practice, this can be helpful for research but also for advertising and polling firms. Your first task will therefore be to implement the k-means clustering you are already familiar with from the lecture.\n",
    "\n",
    "### k-Means Algorithm\n",
    "The algorithm's goal is to partition the data points (also referred to as _observations_) into k sets while minimizing the within-cluster sum of squares. The goal of the k-means clustering is to minimize the objective function\n",
    "$$\\sum_{k=1}^{K} \\sum_{\\{n|x_n \\in C_k \\}} \\|x_n - \\mu_k\\|^2$$\n",
    "\n",
    "such that $K$ is the number of clusters, $x_n$ is the $n$-th point that belongs to the $k$-th cluster, and $\\mu_k$ is the centroid (or _prototype_) of the $k$-th cluster.\n",
    "    \n",
    "The algorithm's implementation proceeds as follows:\n",
    "* Assign four random centroids (starting prototypes)\n",
    "* Assign data points (the users) to the nearest centroid\n",
    "* Recompute the centroid values - the new value of the $k$-th centroid is the average of the points currently assigned to that centroid\n",
    "* Repeat from point 3 until the centroids' values do not change anymore\n",
    "\n",
    "**Write a Python program that computes the k-means clustering for the given dataset with a k value of 4 by following the tasks below.** The output of your program should be a dictionary (or whatever data type works best for you) that assigns a cluster ID (0, 1, 2, 3) to every user in the input file. The first argument in that tuple should be the users's name and the second argument should be the centroid ID to which this user is associated to, e.g. ('user111', 3).<br>\n",
    "After the clustering with k=4 is implemented, run the code with the following starting prototypes for testing your solution: **{0: [9, 33, 29, 25], 1: [4, 44, 12, 41], 2: [10, 13, 44, 65], 3: [10, 44, 48, 70]}**\n",
    "\n",
    "**Notes:**\n",
    "* The return value should be the final centroid values that do not change anymore.\n",
    "* If you happen to come across any empty clusters in your implementation (e.g. if a clusters did not get assigned any data points), they need to be reinitialized in order to return all k clusters later. A common way to do this is using one or more random points far away from their centroid. In our case, it is sufficient to set an empty cluster to all zeros (a value that is located at the data's boundaries), which creates a similar effect. _For the future, keep in mind that this is not always a good idea because it depends on the specific underlying space!_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv, random, queue, numpy as np, igraph as ig\n",
    "from scipy.spatial import distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1: Reading The Data\n",
    "\n",
    "The first thing you are supposed to do is reading in the data set (_NetworkActivityData.csv_). As mentioned above, the file's rows contain a social network user with their activity types (posts, comments, likes and friends). One possibility is to save the data as a dictionary, the user ID as a key and the list of activity types as its values.\n",
    "\n",
    "**Implement a function that reads in a .csv file given a certain path so you can call it later on the data set.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the network activity data set into a dictionary with user IDs as keys\n",
    "def read_data_set(path):\n",
    "    observations_dict = {}\n",
    "    user_id = []\n",
    "    posts = []\n",
    "    comments = []\n",
    "    likes = []\n",
    "    followers = []\n",
    "    \n",
    "    # TODO: Open .csv file\n",
    "    with open(path, encoding='utf-8') as file: \n",
    "        for line in file:\n",
    "            # TODO: Put each row (data for each user) into dictionary of observations\n",
    "            user_id.append(line.split(',')[0])\n",
    "            posts.append(int(line.split(',')[1]))\n",
    "            comments.append(int(line.split(',')[2]))\n",
    "            likes.append(int(line.split(',')[3]))\n",
    "            followers.append(int(line.strip().split(',')[4]))\n",
    "            \n",
    "        for i in range(0, len(user_id)):\n",
    "            observations_dict[user_id[i]]=(posts[i], comments[i], likes[i], followers[i])\n",
    "\n",
    "    return observations_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2: Starting Centroids\n",
    "In order to use k-means clustering, we need k starting centroids to begin with. There are different ways to do it, one of them is randomly assigning the initial values. For each dimension of the centroid, the random values should fall between the dimension's maximum and minimum value of all the points in the data set.<br>\n",
    "For example in our case, if the dimension \"number of comments\" has a maximum value of 49 and a minimum value of 0, the value assigned to the third component of one centroid should be randomly assigned between 0 and 49. The same for the other features and centroids, of course.\n",
    "\n",
    "**Implement a function that computes k random starting centroids, bounded within the minimum and maximum dimension of the input features.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute k random starting centroids\n",
    "def create_starting_centroids(observations, k):\n",
    "\n",
    "    # TODO: Initialize minimum/maximum with arbitrary observation entry    \n",
    "    mins = list(observations.get('user95'))\n",
    "    maxs = list(observations.get('user95'))\n",
    "\n",
    "    # TODO: Find minimum and maximum of observations in each dimension  \n",
    "    nums = [[],[],[],[]]\n",
    "    for key,v in observations.items():\n",
    "        nums[0].append(v[0])\n",
    "        nums[1].append(v[1])\n",
    "        nums[2].append(v[2])\n",
    "        nums[3].append(v[3])\n",
    "    \n",
    "    mins[0] = min(nums[0])\n",
    "    mins[1] = min(nums[1])\n",
    "    mins[2] = min(nums[2])\n",
    "    mins[3] = min(nums[3])\n",
    "    \n",
    "    maxs[0] = max(nums[0])\n",
    "    maxs[1] = max(nums[1])\n",
    "    maxs[2] = max(nums[2])\n",
    "    maxs[3] = max(nums[3])\n",
    "    # TODO: Create dictionary with four random centroids within the observed space\n",
    "    centroids = {}\n",
    "    i = 0\n",
    "    \n",
    "    while i<k:\n",
    "        centroids[i] = [np.random.randint(mins[0], maxs[0]), np.random.randint(mins[1], maxs[1]),\n",
    "                        np.random.randint(mins[2], maxs[2]), np.random.randint(mins[3], maxs[3])]\n",
    "        i += 1\n",
    "    \n",
    "    return centroids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3: Updating The Clusters\n",
    "The core functionality of the k-means algorithm is to assign each observation (user and their properties) to its new cluster and afterwards to recompute the centroid average. These two steps are repeated until the centroids do not change anymore, meaning that the clusters are finally set.\n",
    "\n",
    "Implement two functions, one that **updates the cluster affiliation for each observation by determining the closest centroid** and **another to update the centroid values by averaging their data points**, but only for one iteration in both cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign each of the data points to its closest centroid\n",
    "def update_observation_assignments(observations, centroids, k):\n",
    "    observation_centroids = {}\n",
    "    \n",
    "    # Create dictionary mapping each observation to a centroid index\n",
    "    for key,v in observations.items():\n",
    "        observation_centroids[key] = 0 \n",
    "        centroid_values = centroids[0]\n",
    "        min_dist = float(\"inf\")\n",
    "        # TODO: Get distance to all centroids for current observation\n",
    "        for i in range(0,k):\n",
    "            dist = distance.euclidean(np.array(v), np.array(centroids[i]))\n",
    "            # TODO: Assign centroid that has minimum L2/Euclidean distance\n",
    "            if dist < min_dist:\n",
    "                observation_centroids[key] = i\n",
    "                min_dist = dist\n",
    "                \n",
    "    return observation_centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_centroid_values(observations, observation_centroids, centroids, k):\n",
    "    \n",
    "    # TODO: Initialize centroid\n",
    "    centroid = {}\n",
    "    oc_inverse = {}\n",
    "    \n",
    "    # TODO: Accumulate observations in each cluster\n",
    "    for i in range(k):\n",
    "        oc_inverse.setdefault(i,[])\n",
    "    \n",
    "    for user,center in observation_centroids.items():\n",
    "        oc_inverse.setdefault(center,[]).append(user)\n",
    "    \n",
    "    # TODO: Get cluster mean\n",
    "    for i in range(k):\n",
    "        num_user = len(oc_inverse[i])\n",
    "        num_post = 0\n",
    "        num_comment = 0\n",
    "        num_like = 0\n",
    "        num_friend = 0\n",
    "        for user in oc_inverse[i]:\n",
    "            num_post += observations[user][0]\n",
    "            num_comment += observations[user][1]\n",
    "            num_like += observations[user][2]\n",
    "            num_friend += observations[user][3]\n",
    "        if num_user == 0:\n",
    "            num_user = 1\n",
    "        centroid[i] = [num_post/float(num_user),num_comment/float(num_user),\n",
    "                       num_like/float(num_user),num_friend/float(num_user)]\n",
    "    return centroid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4: k-Means Loop\n",
    "As a last step, k-means uses the functionality that was implemented in the tasks beforehand in a loop and terminates after a few iterations when the centroid values do not change anymore.\n",
    "\n",
    "**Implement k-means clustering by iteratively using the functions defined above until the centroids are set. Afterwards, run your implementation with k=4 using the given starting centroids {0: [9, 33, 29, 25], 1: [4, 44, 12, 41], 2: [10, 13, 44, 65], 3: [10, 44, 48, 70]} instead of the random initialization.**\n",
    "\n",
    "**Hint:** You can verfiy your solution's correctness by comparing your final cluster observations to the pre-defined function's output:<br>\n",
    "`from sklearn.cluster import KMeans`<br>\n",
    "`from numpy import genfromtxt`<br>\n",
    "<br>\n",
    "`observations = genfromtxt('NetworkActivityData.csv', delimiter=',')[:, 1 : 5]`<br>\n",
    "`centers = np.asarray([[9, 33, 29, 25], [4, 44, 12, 41], [10, 13, 44, 65], [10, 44, 48, 70]])`<br>\n",
    "`kmeans = KMeans(n_clusters=4, init=centers).fit(observations)`<br>\n",
    "`result = kmeans.predict(observations)`<br>\n",
    "`print(sum(result == 0), sum(result == 1), sum(result == 2), sum(result == 3))`\n",
    "    \n",
    "**Output the observation counts in each cluster. Finally, have a look at the results and describe the common properties of the four groups.** What information do we have on activity patterns for different groups and for different features? How much did they contribute in the social network? Don't write more than 5 sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "87 212 43 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.4/dist-packages/sklearn/cluster/k_means_.py:971: RuntimeWarning: Explicit initial center position passed: performing only one init in k-means instead of n_init=10\n",
      "  return_n_iter=True)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from numpy import genfromtxt\n",
    "\n",
    "observations = genfromtxt('NetworkActivityData.csv', delimiter=',')[:, 1 : 5]\n",
    "centers = np.asarray([[9, 33, 29, 25], [4, 44, 12, 41], [10, 13, 44, 65], [10, 44, 48, 70]])\n",
    "kmeans = KMeans(n_clusters=4, init=centers).fit(observations)\n",
    "result = kmeans.predict(observations)\n",
    "print(sum(result == 0), sum(result == 1), sum(result == 2), sum(result == 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final assignments: {'user96': 0, 'user194': 0, 'user134': 1, 'user178': 1, 'user56': 1, 'user162': 0, 'user29': 1, 'user12': 1, 'user314': 1, 'user127': 0, 'user142': 1, 'user35': 1, 'user182': 1, 'user69': 0, 'user260': 1, 'user311': 1, 'user148': 2, 'user221': 0, 'user267': 1, 'user342': 1, 'user335': 1, 'user337': 1, 'user14': 1, 'user198': 1, 'user90': 1, 'user304': 1, 'user102': 3, 'user88': 0, 'user49': 1, 'user204': 0, 'user189': 1, 'user282': 1, 'user131': 2, 'user252': 1, 'user68': 2, 'user115': 0, 'user347': 2, 'user60': 1, 'user169': 1, 'user150': 0, 'user15': 2, 'user85': 1, 'user62': 2, 'user217': 1, 'user32': 1, 'user321': 0, 'user275': 1, 'user244': 1, 'user341': 1, 'user159': 2, 'user206': 2, 'user76': 1, 'user157': 1, 'user330': 1, 'user334': 1, 'user122': 1, 'user110': 1, 'user51': 0, 'user338': 2, 'user283': 1, 'user307': 0, 'user207': 1, 'user328': 0, 'user211': 1, 'user37': 0, 'user290': 0, 'user180': 0, 'user43': 0, 'user280': 1, 'user165': 1, 'user103': 2, 'user0': 1, 'user113': 1, 'user226': 1, 'user240': 1, 'user80': 1, 'user288': 1, 'user16': 0, 'user83': 0, 'user294': 0, 'user144': 1, 'user331': 1, 'user120': 1, 'user293': 0, 'user188': 0, 'user73': 3, 'user247': 1, 'user277': 1, 'user197': 0, 'user147': 0, 'user172': 1, 'user11': 1, 'user28': 0, 'user303': 1, 'user308': 0, 'user306': 2, 'user268': 1, 'user89': 1, 'user250': 1, 'user3': 1, 'user279': 1, 'user257': 1, 'user75': 1, 'user40': 1, 'user258': 3, 'user2': 2, 'user105': 0, 'user123': 1, 'user111': 1, 'user152': 1, 'user158': 1, 'user121': 1, 'user66': 1, 'user332': 0, 'user176': 3, 'user213': 1, 'user21': 1, 'user276': 0, 'user344': 1, 'user17': 1, 'user184': 1, 'user173': 1, 'user249': 0, 'user8': 1, 'user305': 1, 'user41': 1, 'user30': 2, 'user20': 0, 'user61': 0, 'user77': 0, 'user349': 1, 'user205': 2, 'user238': 2, 'user259': 1, 'user4': 2, 'user183': 1, 'user200': 1, 'user236': 1, 'user216': 1, 'user179': 0, 'user136': 1, 'user130': 0, 'user284': 1, 'user234': 0, 'user132': 1, 'user220': 1, 'user112': 2, 'user58': 1, 'user107': 1, 'user92': 2, 'user185': 0, 'user27': 2, 'user312': 1, 'user52': 0, 'user253': 2, 'user264': 1, 'user79': 0, 'user286': 1, 'user309': 1, 'user50': 1, 'user114': 1, 'user7': 1, 'user248': 3, 'user222': 0, 'user346': 1, 'user175': 1, 'user95': 1, 'user6': 0, 'user242': 1, 'user45': 0, 'user325': 1, 'user274': 1, 'user34': 1, 'user166': 1, 'user48': 1, 'user141': 0, 'user214': 2, 'user125': 1, 'user5': 1, 'user219': 0, 'user297': 1, 'user285': 0, 'user291': 0, 'user128': 1, 'user170': 1, 'user296': 0, 'user231': 1, 'user199': 3, 'user63': 2, 'user202': 1, 'user210': 1, 'user44': 1, 'user181': 1, 'user149': 1, 'user84': 1, 'user215': 1, 'user313': 0, 'user322': 1, 'user324': 2, 'user118': 0, 'user281': 1, 'user33': 1, 'user64': 1, 'user243': 0, 'user9': 1, 'user339': 0, 'user19': 1, 'user101': 1, 'user340': 1, 'user139': 1, 'user100': 1, 'user156': 1, 'user70': 1, 'user126': 1, 'user138': 1, 'user218': 1, 'user266': 1, 'user91': 0, 'user310': 2, 'user23': 3, 'user164': 0, 'user345': 1, 'user302': 1, 'user145': 2, 'user53': 0, 'user82': 0, 'user287': 1, 'user256': 0, 'user87': 1, 'user36': 1, 'user38': 0, 'user174': 1, 'user109': 0, 'user225': 0, 'user326': 1, 'user117': 1, 'user195': 1, 'user25': 0, 'user177': 1, 'user191': 1, 'user161': 0, 'user203': 1, 'user171': 2, 'user292': 2, 'user270': 0, 'user22': 1, 'user78': 1, 'user18': 0, 'user1': 1, 'user39': 0, 'user227': 1, 'user116': 1, 'user137': 2, 'user235': 1, 'user108': 1, 'user57': 1, 'user99': 1, 'user316': 1, 'user263': 0, 'user278': 0, 'user318': 1, 'user86': 1, 'user167': 1, 'user223': 2, 'user201': 0, 'user261': 0, 'user196': 0, 'user245': 1, 'user209': 0, 'user323': 2, 'user155': 0, 'user93': 1, 'user71': 1, 'user301': 1, 'user255': 1, 'user119': 3, 'user13': 1, 'user208': 0, 'user237': 1, 'user232': 2, 'user239': 0, 'user54': 0, 'user241': 1, 'user315': 1, 'user229': 1, 'user299': 2, 'user154': 1, 'user98': 0, 'user289': 1, 'user163': 1, 'user47': 1, 'user42': 0, 'user254': 0, 'user146': 0, 'user327': 1, 'user94': 1, 'user133': 1, 'user186': 0, 'user212': 1, 'user230': 2, 'user319': 1, 'user295': 1, 'user329': 1, 'user300': 2, 'user67': 2, 'user160': 1, 'user336': 2, 'user59': 0, 'user228': 1, 'user151': 0, 'user262': 0, 'user81': 1, 'user317': 1, 'user272': 1, 'user265': 2, 'user104': 1, 'user192': 1, 'user320': 1, 'user55': 1, 'user143': 1, 'user269': 1, 'user251': 1, 'user46': 1, 'user187': 1, 'user333': 1, 'user72': 2, 'user129': 1, 'user273': 1, 'user224': 2, 'user24': 1, 'user135': 1, 'user246': 1, 'user124': 1, 'user233': 0, 'user26': 2, 'user168': 0, 'user298': 1, 'user97': 1, 'user31': 1, 'user106': 2, 'user348': 0, 'user193': 1, 'user271': 2, 'user153': 1, 'user343': 0, 'user140': 1, 'user65': 2, 'user10': 1, 'user190': 1, 'user74': 1}\n",
      "Observation counts of cluster 0 : 87\n",
      "Observation counts of cluster 1 : 212\n",
      "Observation counts of cluster 2 : 43\n",
      "Observation counts of cluster 3 : 8\n",
      "Average of cluster  0 : [6.344827586206897, 10.816091954022989, 17.011494252873565, 29.103448275862068]\n",
      "Average of cluster  1 : [1.0471698113207548, 2.4764150943396226, 7.415094339622642, 9.61320754716981]\n",
      "Average of cluster  2 : [14.348837209302326, 25.58139534883721, 34.97674418604651, 54.51162790697674]\n",
      "Average of cluster  3 : [25.0, 43.0, 59.375, 84.375]\n"
     ]
    }
   ],
   "source": [
    "# Implementation of k-means clustering\n",
    "def calculate_k_means_clustering(data_set_path, k):\n",
    "    observations = read_data_set(data_set_path)\n",
    "    \n",
    "    #centroids = create_starting_centroids(observations, centroids, k)\n",
    "    centroids = {0: [9, 33, 29, 25], 1: [4, 44, 12, 41], 2: [10, 13, 44, 65], 3: [10, 44, 48, 70]}\n",
    "    \n",
    "    observation_centroids = update_observation_assignments(observations, centroids, k)\n",
    "    new_centroids = update_centroid_values(observations, observation_centroids, centroids, k)\n",
    "\n",
    "    # TODO: Loop until centroids do not change anymore\n",
    "    while new_centroids != centroids:\n",
    "        centroids = new_centroids\n",
    "        \n",
    "        # TODO: Update cluster assignments and centroid values by calling the functions\n",
    "        observation_centroids = update_observation_assignments(observations, centroids, k)\n",
    "        new_centroids = update_centroid_values(observations, observation_centroids, centroids, k)\n",
    "    \n",
    "    print('Final assignments:', observation_centroids)\n",
    "    \n",
    "    observation_count = [0] * 4\n",
    "    # TODO: Count observations per cluster and output results\n",
    "    obs = [[]] * k\n",
    "    for cluster in range(k):\n",
    "        obs[cluster] = []\n",
    "        for key, val in observation_centroids.items():\n",
    "            if val == cluster:\n",
    "                obs[cluster].append(key)\n",
    "        observation_count[cluster] = len(obs[cluster])\n",
    "        \n",
    "        print('Observation counts of cluster',cluster, ':', observation_count[cluster])\n",
    "    \n",
    "    for i in range(0, k):\n",
    "        print('Average of cluster ', i, ':', new_centroids[i])\n",
    "\n",
    "\n",
    "# Run k-means clustering\n",
    "calculate_k_means_clustering('NetworkActivityData.csv', 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 5.2: Girvan-Newman Clustering \n",
    "In this section of the exercise, you will look at another clustering method from the lecture, the Girvan-Newman algorithm. It is an efficient algorithm for computing graph clustering. Your task will be to implement it.\n",
    "\n",
    "### Girvan-Newman Algorithm\n",
    "The central goal of the algorithm is to remove edges according to edge betweenness until the best clustering is obtained, based on modularity as an optimality criterion. You already know that a large betweenness value (for a node) indicates that the corresponding edge is a bridge between two communities in the graph. Cutting that edge means isolating these and yielding separate clusters. The modularity is given by the following formula:<br>\n",
    "$$ Q = \\sum_{i} (e_{ii} - a_i^2) $$\n",
    "with $e_{ii}$ being the number of edges that connect nodes within the $i$-th cluster, and $a_i$ being the fraction of edges that connect nodes from outside to nodes of the $i$-th cluster.\n",
    "\n",
    "The algorithm's implementation proceeds as follows:\n",
    "* Calculate the edge betweenness for all edges in the graph\n",
    "* Remove the edge with highest betweenness\n",
    "* Calculate (and update) the modularity for the resulting clustering\n",
    "* Repeat steps 1 to 3 until there are no edges left\n",
    "\n",
    "**Write a Python program that computes the optimal graph clustering for the Krackhardt Kite graph by completing the tasks below.** The program's input is the imported Krackhardt Kite as an igraph Graph object. The output should be a tuple of arguments, the value of the optimal modularity value and the corresponding Graph object representing the best clustering. You do _not_ have to calculate the edge betweenness yourself, you can use igraph's built-in function. Nevertheless, you are free to do it as an optional task (see below).\n",
    "\n",
    "### Task 1: Modularity\n",
    "In order to compare different clusterings later, the first thing to do is **implementing a function for the modularity calculation** by means of the formula given above. The idea is, for each cluster $i$, to calulcate the fraction of edges within the cluster and the fraction of edges that connects to the $i$-th cluster from outside it.\n",
    "\n",
    "**Notes:**\n",
    "* It may be helpful to look at the lecture's slides on the topic again for understanding what the formula stands for.\n",
    "* You are free to implement everything on your own (as long as the result is correct), but a possible way to solve it is by working with dictionaries of the original and current graph's degrees.\n",
    "* It is useful to add an attribute 'name' to the original graph's nodes in order to preserve the indices as they are reset when creating subgraphs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns a dictionary that maps each node to its degree\n",
    "def calculate_node_degrees(g):\n",
    "    deg_dict = {}\n",
    "    node_list = []\n",
    "    degree = []\n",
    "    \n",
    "    # TODO: # Map calculcate degree centrality\n",
    "    for v in range(g.vcount()):\n",
    "        node_list.append(v)\n",
    "        neighbor = g.neighbors(vertex=v)\n",
    "        degree.append(len(neighbor))\n",
    "        \n",
    "    deg_dict = dict(zip(node_list, degree)) \n",
    "        \n",
    "    return deg_dict\n",
    "\n",
    "\n",
    "# Calculates the modularity of graph g using the original and current degree mappings\n",
    "def calculate_modularity(g, orig_deg_dict, num_edges):\n",
    "    Q = 0\n",
    "    components = g.components()\n",
    "    deg_dict = calculate_node_degrees(g)\n",
    "    \n",
    "    # Loop over clusters and count edges\n",
    "    for i in range(len(components)):\n",
    "        subgraph = components.subgraph(i)\n",
    "        e = 0 # Fraction of edges within cluster\n",
    "        a = 0 # Fraction of edges between in-cluster nodes and nodes outside\n",
    "                \n",
    "        e = subgraph.ecount()# TODO: Get the current degree dict entry, pay attention to indices\n",
    "        for v in range(len(subgraph.vs[\"name\"])):\n",
    "            a += orig_deg_dict[subgraph.vs[\"name\"][v]]# TODO: Get the original degree dict entry, pay attention to indices        \n",
    "        \n",
    "        Q += e/num_edges - np.square(a/(2*num_edges))\n",
    "\n",
    "    return Q\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2: Girvan-Newman Loop\n",
    "The next step is to implement the core functionality. Determine the **betweenness values for all edges** in the current graph first and remove the one with the highest betweenness value to get a meaningful clustering. Additionally, calculate the **graph's modularity value using the function defined above and keep track of the largest one**. This is repeated until no edges are left anymore. The final output is the graph with the best clustering and the corresponding modularity value.\n",
    "\n",
    "In order to test your implementation, compare your results to the built-in functions' solution. The  function `community_edge_betweenness()` returns a dendrogram whose clustering (obtained by using `as_clustering()` on it) you can plot. If you call a graph's `modularity()` function with that clustering, you can check the optimal modularity value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Graph\n",
      " IGRAPH U--- 10 18 --\n",
      "+ edges:\n",
      " 0 --  1  2  3  5          4 --  1  3  6             8 --  7  9\n",
      " 1 --  0  3  4  6          5 --  0  2  3  6  7       9 --  8\n",
      " 2 --  0  3  5             6 --  1  3  4  5  7\n",
      " 3 --  0  1  2  4  5  6    7 --  5  6  8\n",
      "\n",
      "Largest modularity value:  0.18055555555555552\n",
      "\n",
      "Clustered Graph\n",
      " IGRAPH UN-- 10 10 --\n",
      "+ attr: name (v)\n",
      "+ edges (vertex names):\n",
      "0--2, 0--5, 1--3, 1--4, 1--6, 2--5, 3--4, 3--6, 4--6, 8--9\n"
     ]
    },
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"600pt\" height=\"600pt\" viewBox=\"0 0 600 600\" version=\"1.1\">\n",
       "<g id=\"surface470\">\n",
       "<rect x=\"0\" y=\"0\" width=\"600\" height=\"600\" style=\"fill:rgb(100%,100%,100%);fill-opacity:1;stroke:none;\"/>\n",
       "<path style=\"fill:none;stroke-width:1;stroke-linecap:butt;stroke-linejoin:miter;stroke:rgb(26.666667%,26.666667%,26.666667%);stroke-opacity:1;stroke-miterlimit:10;\" d=\"M 190.074219 123.269531 L 134.117188 29.644531 \"/>\n",
       "<path style=\"fill:none;stroke-width:1;stroke-linecap:butt;stroke-linejoin:miter;stroke:rgb(26.666667%,26.666667%,26.666667%);stroke-opacity:1;stroke-miterlimit:10;\" d=\"M 190.074219 123.269531 L 260.8125 20 \"/>\n",
       "<path style=\"fill:none;stroke-width:1;stroke-linecap:butt;stroke-linejoin:miter;stroke:rgb(26.666667%,26.666667%,26.666667%);stroke-opacity:1;stroke-miterlimit:10;\" d=\"M 580 301.699219 L 435.253906 305.335938 \"/>\n",
       "<path style=\"fill:none;stroke-width:1;stroke-linecap:butt;stroke-linejoin:miter;stroke:rgb(26.666667%,26.666667%,26.666667%);stroke-opacity:1;stroke-miterlimit:10;\" d=\"M 580 301.699219 L 508.375 379.726562 \"/>\n",
       "<path style=\"fill:none;stroke-width:1;stroke-linecap:butt;stroke-linejoin:miter;stroke:rgb(26.666667%,26.666667%,26.666667%);stroke-opacity:1;stroke-miterlimit:10;\" d=\"M 580 301.699219 L 510.753906 225.527344 \"/>\n",
       "<path style=\"fill:none;stroke-width:1;stroke-linecap:butt;stroke-linejoin:miter;stroke:rgb(26.666667%,26.666667%,26.666667%);stroke-opacity:1;stroke-miterlimit:10;\" d=\"M 134.117188 29.644531 L 260.8125 20 \"/>\n",
       "<path style=\"fill:none;stroke-width:1;stroke-linecap:butt;stroke-linejoin:miter;stroke:rgb(26.666667%,26.666667%,26.666667%);stroke-opacity:1;stroke-miterlimit:10;\" d=\"M 435.253906 305.335938 L 508.375 379.726562 \"/>\n",
       "<path style=\"fill:none;stroke-width:1;stroke-linecap:butt;stroke-linejoin:miter;stroke:rgb(26.666667%,26.666667%,26.666667%);stroke-opacity:1;stroke-miterlimit:10;\" d=\"M 435.253906 305.335938 L 510.753906 225.527344 \"/>\n",
       "<path style=\"fill:none;stroke-width:1;stroke-linecap:butt;stroke-linejoin:miter;stroke:rgb(26.666667%,26.666667%,26.666667%);stroke-opacity:1;stroke-miterlimit:10;\" d=\"M 508.375 379.726562 L 510.753906 225.527344 \"/>\n",
       "<path style=\"fill:none;stroke-width:1;stroke-linecap:butt;stroke-linejoin:miter;stroke:rgb(26.666667%,26.666667%,26.666667%);stroke-opacity:1;stroke-miterlimit:10;\" d=\"M 81.050781 450.429688 L 20 350.695312 \"/>\n",
       "<path style=\"fill-rule:nonzero;fill:rgb(100%,0%,0%);fill-opacity:1;stroke-width:1;stroke-linecap:butt;stroke-linejoin:miter;stroke:rgb(0%,0%,0%);stroke-opacity:1;stroke-miterlimit:10;\" d=\"M 200.074219 123.269531 C 200.074219 128.792969 195.597656 133.269531 190.074219 133.269531 C 184.550781 133.269531 180.074219 128.792969 180.074219 123.269531 C 180.074219 117.746094 184.550781 113.269531 190.074219 113.269531 C 195.597656 113.269531 200.074219 117.746094 200.074219 123.269531 \"/>\n",
       "<path style=\"fill-rule:nonzero;fill:rgb(100%,0%,0%);fill-opacity:1;stroke-width:1;stroke-linecap:butt;stroke-linejoin:miter;stroke:rgb(0%,0%,0%);stroke-opacity:1;stroke-miterlimit:10;\" d=\"M 590 301.699219 C 590 307.21875 585.523438 311.699219 580 311.699219 C 574.476562 311.699219 570 307.21875 570 301.699219 C 570 296.175781 574.476562 291.699219 580 291.699219 C 585.523438 291.699219 590 296.175781 590 301.699219 \"/>\n",
       "<path style=\"fill-rule:nonzero;fill:rgb(100%,0%,0%);fill-opacity:1;stroke-width:1;stroke-linecap:butt;stroke-linejoin:miter;stroke:rgb(0%,0%,0%);stroke-opacity:1;stroke-miterlimit:10;\" d=\"M 144.117188 29.644531 C 144.117188 35.164062 139.640625 39.644531 134.117188 39.644531 C 128.59375 39.644531 124.117188 35.164062 124.117188 29.644531 C 124.117188 24.121094 128.59375 19.644531 134.117188 19.644531 C 139.640625 19.644531 144.117188 24.121094 144.117188 29.644531 \"/>\n",
       "<path style=\"fill-rule:nonzero;fill:rgb(100%,0%,0%);fill-opacity:1;stroke-width:1;stroke-linecap:butt;stroke-linejoin:miter;stroke:rgb(0%,0%,0%);stroke-opacity:1;stroke-miterlimit:10;\" d=\"M 445.253906 305.335938 C 445.253906 310.859375 440.777344 315.335938 435.253906 315.335938 C 429.730469 315.335938 425.253906 310.859375 425.253906 305.335938 C 425.253906 299.816406 429.730469 295.335938 435.253906 295.335938 C 440.777344 295.335938 445.253906 299.816406 445.253906 305.335938 \"/>\n",
       "<path style=\"fill-rule:nonzero;fill:rgb(100%,0%,0%);fill-opacity:1;stroke-width:1;stroke-linecap:butt;stroke-linejoin:miter;stroke:rgb(0%,0%,0%);stroke-opacity:1;stroke-miterlimit:10;\" d=\"M 518.375 379.726562 C 518.375 385.25 513.898438 389.726562 508.375 389.726562 C 502.855469 389.726562 498.375 385.25 498.375 379.726562 C 498.375 374.207031 502.855469 369.726562 508.375 369.726562 C 513.898438 369.726562 518.375 374.207031 518.375 379.726562 \"/>\n",
       "<path style=\"fill-rule:nonzero;fill:rgb(100%,0%,0%);fill-opacity:1;stroke-width:1;stroke-linecap:butt;stroke-linejoin:miter;stroke:rgb(0%,0%,0%);stroke-opacity:1;stroke-miterlimit:10;\" d=\"M 270.8125 20 C 270.8125 25.523438 266.335938 30 260.8125 30 C 255.289062 30 250.8125 25.523438 250.8125 20 C 250.8125 14.476562 255.289062 10 260.8125 10 C 266.335938 10 270.8125 14.476562 270.8125 20 \"/>\n",
       "<path style=\"fill-rule:nonzero;fill:rgb(100%,0%,0%);fill-opacity:1;stroke-width:1;stroke-linecap:butt;stroke-linejoin:miter;stroke:rgb(0%,0%,0%);stroke-opacity:1;stroke-miterlimit:10;\" d=\"M 520.753906 225.527344 C 520.753906 231.050781 516.277344 235.527344 510.753906 235.527344 C 505.230469 235.527344 500.753906 231.050781 500.753906 225.527344 C 500.753906 220.003906 505.230469 215.527344 510.753906 215.527344 C 516.277344 215.527344 520.753906 220.003906 520.753906 225.527344 \"/>\n",
       "<path style=\"fill-rule:nonzero;fill:rgb(100%,0%,0%);fill-opacity:1;stroke-width:1;stroke-linecap:butt;stroke-linejoin:miter;stroke:rgb(0%,0%,0%);stroke-opacity:1;stroke-miterlimit:10;\" d=\"M 298.417969 580 C 298.417969 585.523438 293.941406 590 288.417969 590 C 282.894531 590 278.417969 585.523438 278.417969 580 C 278.417969 574.476562 282.894531 570 288.417969 570 C 293.941406 570 298.417969 574.476562 298.417969 580 \"/>\n",
       "<path style=\"fill-rule:nonzero;fill:rgb(100%,0%,0%);fill-opacity:1;stroke-width:1;stroke-linecap:butt;stroke-linejoin:miter;stroke:rgb(0%,0%,0%);stroke-opacity:1;stroke-miterlimit:10;\" d=\"M 91.050781 450.429688 C 91.050781 455.949219 86.574219 460.429688 81.050781 460.429688 C 75.527344 460.429688 71.050781 455.949219 71.050781 450.429688 C 71.050781 444.90625 75.527344 440.429688 81.050781 440.429688 C 86.574219 440.429688 91.050781 444.90625 91.050781 450.429688 \"/>\n",
       "<path style=\"fill-rule:nonzero;fill:rgb(100%,0%,0%);fill-opacity:1;stroke-width:1;stroke-linecap:butt;stroke-linejoin:miter;stroke:rgb(0%,0%,0%);stroke-opacity:1;stroke-miterlimit:10;\" d=\"M 30 350.695312 C 30 356.214844 25.523438 360.695312 20 360.695312 C 14.476562 360.695312 10 356.214844 10 350.695312 C 10 345.171875 14.476562 340.695312 20 340.695312 C 25.523438 340.695312 30 345.171875 30 350.695312 \"/>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<igraph.drawing.Plot at 0x7fce8e44dc88>"
      ]
     },
     "execution_count": 213,
     "metadata": {
      "image/svg+xml": {
       "isolated": true
      }
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Implementation Girvan-Newman clustering\n",
    "def calculate_girvan_newman_clustering(g):\n",
    "    num_edges = g.ecount()# TODO: Get number of edges\n",
    "    largest_Q = 0.0\n",
    "\n",
    "    final_clustering = g.copy()\n",
    "    orig_deg = calculate_node_degrees(g)\n",
    "    orig_g=g.copy() \n",
    "    \n",
    "    # TODO: Preserving original indices as node attributes (for later modularity computation)\n",
    "    for v in g.vs:\n",
    "        v[\"name\"] = v.index\n",
    "        \n",
    "    # TODO: Remove all edges according to their modularity until none are left\n",
    "    edge_bet = g.edge_betweenness()\n",
    "    largest_Q = 0\n",
    "    while g.ecount()>0:\n",
    "        # Compute modularity for remaining edges\n",
    "        cluster =  g.clusters().membership\n",
    "        Q = calculate_modularity(g, orig_deg, num_edges)\n",
    "\n",
    "        if Q >= largest_Q:\n",
    "            # TODO: Update largest modularity value and copy the corresponding graph\n",
    "            largest_Q = Q\n",
    "            final_clustering = g.copy()\n",
    "        \n",
    "        # TODO: Compute betweenness for each edge and get highest index\n",
    "        edge_bet = g.edge_betweenness()\n",
    "        #print(edge_bet_dict)\n",
    "        max_bet = 0\n",
    "        max_index = 0\n",
    "        for e in range(len(edge_bet)):\n",
    "\n",
    "            if edge_bet[e] > max_bet:\n",
    "                max_bet = edge_bet[e]\n",
    "                max_index = e\n",
    "                \n",
    "        # TODO: Delete edge with highest index        \n",
    "        g.delete_edges(max_index)\n",
    "    \n",
    "    return final_clustering, largest_Q\n",
    "\n",
    "\n",
    "# Import the Kite\n",
    "g = ig.Graph.Famous('Krackhardt_Kite')\n",
    "print('Original Graph\\n', g)\n",
    "\n",
    "# Compute clustering and largest modularity value\n",
    "graph_clustering, largest_mod = calculate_girvan_newman_clustering(g)\n",
    "print('\\nLargest modularity value: ', largest_mod)\n",
    "print('\\nClustered Graph\\n', graph_clustering)\n",
    "\n",
    "# Plot the clustered graph\n",
    "ig.plot(graph_clustering)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modularity:  0.1805555555555555\n"
     ]
    },
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"600pt\" height=\"600pt\" viewBox=\"0 0 600 600\" version=\"1.1\">\n",
       "<g id=\"surface170\">\n",
       "<rect x=\"0\" y=\"0\" width=\"600\" height=\"600\" style=\"fill:rgb(100%,100%,100%);fill-opacity:1;stroke:none;\"/>\n",
       "<path style=\"fill:none;stroke-width:1;stroke-linecap:butt;stroke-linejoin:miter;stroke:rgb(80%,80%,80%);stroke-opacity:1;stroke-miterlimit:10;\" d=\"M 415.886719 580 L 148.148438 560.863281 \"/>\n",
       "<path style=\"fill:none;stroke-width:1;stroke-linecap:butt;stroke-linejoin:miter;stroke:rgb(20%,20%,20%);stroke-opacity:1;stroke-miterlimit:10;\" d=\"M 415.886719 580 L 580 513.742188 \"/>\n",
       "<path style=\"fill:none;stroke-width:1;stroke-linecap:butt;stroke-linejoin:miter;stroke:rgb(80%,80%,80%);stroke-opacity:1;stroke-miterlimit:10;\" d=\"M 415.886719 580 L 298.464844 500.273438 \"/>\n",
       "<path style=\"fill:none;stroke-width:1;stroke-linecap:butt;stroke-linejoin:miter;stroke:rgb(20%,20%,20%);stroke-opacity:1;stroke-miterlimit:10;\" d=\"M 415.886719 580 L 449.53125 415.351562 \"/>\n",
       "<path style=\"fill:none;stroke-width:1;stroke-linecap:butt;stroke-linejoin:miter;stroke:rgb(20%,20%,20%);stroke-opacity:1;stroke-miterlimit:10;\" d=\"M 148.148438 560.863281 L 298.464844 500.273438 \"/>\n",
       "<path style=\"fill:none;stroke-width:1;stroke-linecap:butt;stroke-linejoin:miter;stroke:rgb(20%,20%,20%);stroke-opacity:1;stroke-miterlimit:10;\" d=\"M 148.148438 560.863281 L 20 473.714844 \"/>\n",
       "<path style=\"fill:none;stroke-width:1;stroke-linecap:butt;stroke-linejoin:miter;stroke:rgb(20%,20%,20%);stroke-opacity:1;stroke-miterlimit:10;\" d=\"M 148.148438 560.863281 L 191.542969 396.910156 \"/>\n",
       "<path style=\"fill:none;stroke-width:1;stroke-linecap:butt;stroke-linejoin:miter;stroke:rgb(80%,80%,80%);stroke-opacity:1;stroke-miterlimit:10;\" d=\"M 580 513.742188 L 298.464844 500.273438 \"/>\n",
       "<path style=\"fill:none;stroke-width:1;stroke-linecap:butt;stroke-linejoin:miter;stroke:rgb(20%,20%,20%);stroke-opacity:1;stroke-miterlimit:10;\" d=\"M 580 513.742188 L 449.53125 415.351562 \"/>\n",
       "<path style=\"fill:none;stroke-width:1;stroke-linecap:butt;stroke-linejoin:miter;stroke:rgb(20%,20%,20%);stroke-opacity:1;stroke-miterlimit:10;\" d=\"M 298.464844 500.273438 L 20 473.714844 \"/>\n",
       "<path style=\"fill:none;stroke-width:1;stroke-linecap:butt;stroke-linejoin:miter;stroke:rgb(80%,80%,80%);stroke-opacity:1;stroke-miterlimit:10;\" d=\"M 298.464844 500.273438 L 449.53125 415.351562 \"/>\n",
       "<path style=\"fill:none;stroke-width:1;stroke-linecap:butt;stroke-linejoin:miter;stroke:rgb(20%,20%,20%);stroke-opacity:1;stroke-miterlimit:10;\" d=\"M 298.464844 500.273438 L 191.542969 396.910156 \"/>\n",
       "<path style=\"fill:none;stroke-width:1;stroke-linecap:butt;stroke-linejoin:miter;stroke:rgb(20%,20%,20%);stroke-opacity:1;stroke-miterlimit:10;\" d=\"M 20 473.714844 L 191.542969 396.910156 \"/>\n",
       "<path style=\"fill:none;stroke-width:1;stroke-linecap:butt;stroke-linejoin:miter;stroke:rgb(80%,80%,80%);stroke-opacity:1;stroke-miterlimit:10;\" d=\"M 449.53125 415.351562 L 191.542969 396.910156 \"/>\n",
       "<path style=\"fill:none;stroke-width:1;stroke-linecap:butt;stroke-linejoin:miter;stroke:rgb(80%,80%,80%);stroke-opacity:1;stroke-miterlimit:10;\" d=\"M 449.53125 415.351562 L 347.90625 289.386719 \"/>\n",
       "<path style=\"fill:none;stroke-width:1;stroke-linecap:butt;stroke-linejoin:miter;stroke:rgb(80%,80%,80%);stroke-opacity:1;stroke-miterlimit:10;\" d=\"M 191.542969 396.910156 L 347.90625 289.386719 \"/>\n",
       "<path style=\"fill:none;stroke-width:1;stroke-linecap:butt;stroke-linejoin:miter;stroke:rgb(80%,80%,80%);stroke-opacity:1;stroke-miterlimit:10;\" d=\"M 347.90625 289.386719 L 379.863281 153.082031 \"/>\n",
       "<path style=\"fill:none;stroke-width:1;stroke-linecap:butt;stroke-linejoin:miter;stroke:rgb(20%,20%,20%);stroke-opacity:1;stroke-miterlimit:10;\" d=\"M 379.863281 153.082031 L 411.0625 20 \"/>\n",
       "<path style=\"fill-rule:nonzero;fill:rgb(100%,0%,0%);fill-opacity:1;stroke-width:1;stroke-linecap:butt;stroke-linejoin:miter;stroke:rgb(0%,0%,0%);stroke-opacity:1;stroke-miterlimit:10;\" d=\"M 425.886719 580 C 425.886719 585.523438 421.410156 590 415.886719 590 C 410.363281 590 405.886719 585.523438 405.886719 580 C 405.886719 574.476562 410.363281 570 415.886719 570 C 421.410156 570 425.886719 574.476562 425.886719 580 \"/>\n",
       "<path style=\"fill-rule:nonzero;fill:rgb(0%,100%,0%);fill-opacity:1;stroke-width:1;stroke-linecap:butt;stroke-linejoin:miter;stroke:rgb(0%,0%,0%);stroke-opacity:1;stroke-miterlimit:10;\" d=\"M 158.148438 560.863281 C 158.148438 566.382812 153.671875 570.863281 148.148438 570.863281 C 142.625 570.863281 138.148438 566.382812 138.148438 560.863281 C 138.148438 555.339844 142.625 550.863281 148.148438 550.863281 C 153.671875 550.863281 158.148438 555.339844 158.148438 560.863281 \"/>\n",
       "<path style=\"fill-rule:nonzero;fill:rgb(100%,0%,0%);fill-opacity:1;stroke-width:1;stroke-linecap:butt;stroke-linejoin:miter;stroke:rgb(0%,0%,0%);stroke-opacity:1;stroke-miterlimit:10;\" d=\"M 590 513.742188 C 590 519.265625 585.523438 523.742188 580 523.742188 C 574.476562 523.742188 570 519.265625 570 513.742188 C 570 508.21875 574.476562 503.742188 580 503.742188 C 585.523438 503.742188 590 508.21875 590 513.742188 \"/>\n",
       "<path style=\"fill-rule:nonzero;fill:rgb(0%,100%,0%);fill-opacity:1;stroke-width:1;stroke-linecap:butt;stroke-linejoin:miter;stroke:rgb(0%,0%,0%);stroke-opacity:1;stroke-miterlimit:10;\" d=\"M 308.464844 500.273438 C 308.464844 505.796875 303.988281 510.273438 298.464844 510.273438 C 292.941406 510.273438 288.464844 505.796875 288.464844 500.273438 C 288.464844 494.75 292.941406 490.273438 298.464844 490.273438 C 303.988281 490.273438 308.464844 494.75 308.464844 500.273438 \"/>\n",
       "<path style=\"fill-rule:nonzero;fill:rgb(0%,100%,0%);fill-opacity:1;stroke-width:1;stroke-linecap:butt;stroke-linejoin:miter;stroke:rgb(0%,0%,0%);stroke-opacity:1;stroke-miterlimit:10;\" d=\"M 30 473.714844 C 30 479.238281 25.523438 483.714844 20 483.714844 C 14.476562 483.714844 10 479.238281 10 473.714844 C 10 468.191406 14.476562 463.714844 20 463.714844 C 25.523438 463.714844 30 468.191406 30 473.714844 \"/>\n",
       "<path style=\"fill-rule:nonzero;fill:rgb(100%,0%,0%);fill-opacity:1;stroke-width:1;stroke-linecap:butt;stroke-linejoin:miter;stroke:rgb(0%,0%,0%);stroke-opacity:1;stroke-miterlimit:10;\" d=\"M 459.53125 415.351562 C 459.53125 420.871094 455.054688 425.351562 449.53125 425.351562 C 444.007812 425.351562 439.53125 420.871094 439.53125 415.351562 C 439.53125 409.828125 444.007812 405.351562 449.53125 405.351562 C 455.054688 405.351562 459.53125 409.828125 459.53125 415.351562 \"/>\n",
       "<path style=\"fill-rule:nonzero;fill:rgb(0%,100%,0%);fill-opacity:1;stroke-width:1;stroke-linecap:butt;stroke-linejoin:miter;stroke:rgb(0%,0%,0%);stroke-opacity:1;stroke-miterlimit:10;\" d=\"M 201.542969 396.910156 C 201.542969 402.433594 197.066406 406.910156 191.542969 406.910156 C 186.019531 406.910156 181.542969 402.433594 181.542969 396.910156 C 181.542969 391.386719 186.019531 386.910156 191.542969 386.910156 C 197.066406 386.910156 201.542969 391.386719 201.542969 396.910156 \"/>\n",
       "<path style=\"fill-rule:nonzero;fill:rgb(0%,0%,100%);fill-opacity:1;stroke-width:1;stroke-linecap:butt;stroke-linejoin:miter;stroke:rgb(0%,0%,0%);stroke-opacity:1;stroke-miterlimit:10;\" d=\"M 357.90625 289.386719 C 357.90625 294.910156 353.429688 299.386719 347.90625 299.386719 C 342.382812 299.386719 337.90625 294.910156 337.90625 289.386719 C 337.90625 283.863281 342.382812 279.386719 347.90625 279.386719 C 353.429688 279.386719 357.90625 283.863281 357.90625 289.386719 \"/>\n",
       "<path style=\"fill-rule:nonzero;fill:rgb(100%,100%,0%);fill-opacity:1;stroke-width:1;stroke-linecap:butt;stroke-linejoin:miter;stroke:rgb(0%,0%,0%);stroke-opacity:1;stroke-miterlimit:10;\" d=\"M 389.863281 153.082031 C 389.863281 158.605469 385.382812 163.082031 379.863281 163.082031 C 374.339844 163.082031 369.863281 158.605469 369.863281 153.082031 C 369.863281 147.558594 374.339844 143.082031 379.863281 143.082031 C 385.382812 143.082031 389.863281 147.558594 389.863281 153.082031 \"/>\n",
       "<path style=\"fill-rule:nonzero;fill:rgb(100%,100%,0%);fill-opacity:1;stroke-width:1;stroke-linecap:butt;stroke-linejoin:miter;stroke:rgb(0%,0%,0%);stroke-opacity:1;stroke-miterlimit:10;\" d=\"M 421.0625 20 C 421.0625 25.523438 416.585938 30 411.0625 30 C 405.539062 30 401.0625 25.523438 401.0625 20 C 401.0625 14.476562 405.539062 10 411.0625 10 C 416.585938 10 421.0625 14.476562 421.0625 20 \"/>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<igraph.drawing.Plot at 0x7fcea12e4128>"
      ]
     },
     "execution_count": 105,
     "metadata": {
      "image/svg+xml": {
       "isolated": true
      }
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For testing your implementation\n",
    "g = ig.Graph.Famous('Krackhardt_Kite')\n",
    "\n",
    "# TODO: Show dendrogram, the optimal modularity and corresponding clustering\n",
    "communities = g.community_edge_betweenness()\n",
    "print('Modularity: ', g.modularity(communities.as_clustering()))\n",
    "# TODO: Plot the clustering\n",
    "ig.plot(communities.as_clustering())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (Optional) Task 3: Edge Betweenness\n",
    "As an optional task, you can implement the edge betweenness calculation that is given by the `edge_betweenness()` function above. You **do not** have to do this task in order to get the full grade but you can still improve if you were not able to solve the problems 5.1 or 5.2 sufficiently.\n",
    "    \n",
    "**Implement the edge betweenness based on Brandes' algorithm presented in [1].**\n",
    "\n",
    "[1] U. Brandes: _A Faster Algorithm for Betweenness Centrality._ 2001. ([PDF](https://algo.uni-konstanz.de/publications/b-fabc-01.pdf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_edge_betweenness(g):\n",
    "    C = {} # Edge betweenness dictionary\n",
    "    \n",
    "    # Calculate edge betweenness according to Brandes algorithm\n",
    "    \n",
    "    return C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
